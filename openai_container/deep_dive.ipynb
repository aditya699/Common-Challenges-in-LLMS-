{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking existing containers...\n",
      "SyncCursorPage[ContainerListResponse](data=[ContainerListResponse(id='cntr_68314a508e8081919e5d1051f3fcbe27', created_at=1748060752, name='data-analysis-container', object='container', status='running', expires_after=ExpiresAfter(anchor='last_active_at', minutes=20), last_active_at=1748060947), ContainerListResponse(id='cntr_6831312c4e608191b358bef087b081be', created_at=1748054316, name='test-container', object='container', status='expired', expires_after=ExpiresAfter(anchor='last_active_at', minutes=20), last_active_at=1748060947), ContainerListResponse(id='cntr_68312ae94c608191a7ae842feb3da6cd', created_at=1748052713, name='test-container', object='container', status='expired', expires_after=ExpiresAfter(anchor='last_active_at', minutes=20), last_active_at=1748060947), ContainerListResponse(id='cntr_683129f7152881918ba879d28b5cfca7', created_at=1748052471, name='auto', object='container', status='expired', expires_after=ExpiresAfter(anchor='last_active_at', minutes=20), last_active_at=1748060947)], has_more=False, object='list', first_id='cntr_68314a508e8081919e5d1051f3fcbe27', last_id='cntr_683129f7152881918ba879d28b5cfca7')\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check existing containers and find active ones\n",
    "print(\"Checking existing containers...\")\n",
    "containers = client.containers.list()\n",
    "print(containers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container cntr_68314a508e8081919e5d1051f3fcbe27: running\n",
      "Using active container: cntr_68314a508e8081919e5d1051f3fcbe27\n"
     ]
    }
   ],
   "source": [
    "# Find first active (running) container\n",
    "active_container = None\n",
    "for container in containers.data:\n",
    "    print(f\"Container {container.id}: {container.status}\")\n",
    "    if container.status == \"running\":\n",
    "        active_container = container\n",
    "        break\n",
    "\n",
    "if active_container:\n",
    "    container_id = active_container.id\n",
    "    print(f\"Using active container: {container_id}\")\n",
    "else:\n",
    "    print(\"No active containers found. Creating new one...\")\n",
    "    container = client.containers.create(name=\"data-analysis-container\")\n",
    "    container_id = container.id\n",
    "    print(f\"Created new container: {container_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded: /mnt/data/56942508662ee703512c82c531286d53-data.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Upload file\n",
    "url = f\"https://api.openai.com/v1/containers/{container_id}/files\"\n",
    "headers = {\"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\"}\n",
    "files = {'file': ('data.csv', open('data.csv', 'rb'))}\n",
    "\n",
    "response = requests.post(url, headers=headers, files=files)\n",
    "file_path = response.json()['path']\n",
    "print(f\"File uploaded: {file_path}\")\n",
    "\n",
    "# Step 3: Analyze with code interpreter\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    tools=[{\"type\": \"code_interpreter\", \"container\": container_id}],\n",
    "    tool_choice=\"required\",\n",
    "    input=f\"Analyze CSV at '{file_path}'. How many rows?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The CSV file contains 1000 rows. Would you like me to perform any other analysis on this data?\n",
      "Code executed:\n",
      "import pandas as pd\n",
      "\n",
      "# Load the CSV file\n",
      "file_path = '/mnt/data/56942508662ee703512c82c531286d53-data.csv'\n",
      "data = pd.read_csv(file_path)\n",
      "\n",
      "# Get number of rows\n",
      "num_rows = data.shape[0]\n",
      "num_rows\n",
      "Tokens used: 510\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(f\"Answer: {response.output_text}\")\n",
    "print(f\"Code executed:\\n{response.output[0].code}\")\n",
    "print(f\"Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling charts in openai container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Analyze with code interpreter\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    tools=[{\"type\": \"code_interpreter\", \"container\": container_id}],\n",
    "    tool_choice=\"required\",\n",
    "    input=f\"\"\"Analyze CSV at '{file_path}'. Plot a bar chart showing user breakdown by gender.\n",
    "Rotate x-axis labels by 45 degrees for readability. Increase figure width if needed.\n",
    "Add count labels on top of each bar.\"\"\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated file: cfile_68314d0f620481919d557c0e09a639ef.png (ID: cfile_68314d0f620481919d557c0e09a639ef)\n",
      "Image saved as: gender_breakdown_chart.png\n",
      "\n",
      "Answer: The bar chart shows the user breakdown by gender from the provided CSV file. The x-axis labels have been rotated by 45 degrees for better readability, and the figure width was increased to accommodate the labels. Count labels are displayed on top of each bar for clarity.\n",
      "\n",
      "If you need any further analysis or adjustments, please let me know!\n",
      "Tokens used: 3952\n"
     ]
    }
   ],
   "source": [
    "# Extract file information from the response\n",
    "for output in response.output:\n",
    "    if hasattr(output, 'content'):\n",
    "        for content in output.content:\n",
    "            if hasattr(content, 'annotations'):\n",
    "                for annotation in content.annotations:\n",
    "                    if annotation.type == 'container_file_citation':\n",
    "                        file_id = annotation.file_id\n",
    "                        filename = annotation.filename\n",
    "                        print(f\"Generated file: {filename} (ID: {file_id})\")\n",
    "                        \n",
    "                        # Download the image file\n",
    "                        download_url = f\"https://api.openai.com/v1/containers/{container_id}/files/{file_id}/content\"\n",
    "                        headers = {\"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\"}\n",
    "                        \n",
    "                        file_response = requests.get(download_url, headers=headers)\n",
    "                        if file_response.status_code == 200:\n",
    "                            # Save the image locally\n",
    "                            local_filename = f\"gender_breakdown_chart.png\"\n",
    "                            with open(local_filename, 'wb') as f:\n",
    "                                f.write(file_response.content)\n",
    "                            print(f\"Image saved as: {local_filename}\")\n",
    "                        else:\n",
    "                            print(f\"Failed to download file: {file_response.status_code}\")\n",
    "\n",
    "# Also display the response for reference\n",
    "print(f\"\\nAnswer: {response.output_text}\")\n",
    "print(f\"Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_68314b9a4bc08191990e7d962070b0e20cdb1ad7bd57b962', created_at=1748061082.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseCodeInterpreterToolCall(id='ci_68314b9afc6c81919920e1e5ba0a1e300cdb1ad7bd57b962', code=\"import pandas as pd\\n\\n# Load the CSV file to examine its content\\nfile_path = '/mnt/data/56942508662ee703512c82c531286d53-data.csv'\\ndata = pd.read_csv(file_path)\\n\\n# Display the first few rows and columns to understand its structure\\ndata.head(), data.columns\", results=None, status='completed', type='code_interpreter_call', container_id='cntr_68314a508e8081919e5d1051f3fcbe27', outputs=None), ResponseOutputMessage(id='msg_68314b9d19448191bbae006e2a38f10f0cdb1ad7bd57b962', content=[ResponseOutputText(annotations=[], text='The dataset contains the following columns: id, first_name, last_name, email, gender, and ip_address. I will now plot a chart to show the user breakdown by gender.', type='output_text')], role='assistant', status='completed', type='message'), ResponseCodeInterpreterToolCall(id='ci_68314b9de4c88191becec422098ad3e00cdb1ad7bd57b962', code=\"import matplotlib.pyplot as plt\\n\\n# Count the number of users by gender\\ngender_counts = data['gender'].value_counts()\\n\\n# Plotting the user breakdown by gender\\nplt.figure(figsize=(8, 6))\\ngender_counts.plot(kind='bar', color=['skyblue', 'lightgreen', 'orange'])\\nplt.title('User Breakdown by Gender')\\nplt.xlabel('Gender')\\nplt.ylabel('Number of Users')\\nplt.xticks(rotation=0)\\nplt.show()\", results=None, status='completed', type='code_interpreter_call', container_id='cntr_68314a508e8081919e5d1051f3fcbe27', outputs=None), ResponseOutputMessage(id='msg_68314ba21af4819189e140bfeb38ac530cdb1ad7bd57b962', content=[ResponseOutputText(annotations=[AnnotationFileCitation(file_id='cfile_68314ba182008191ac8c3503d924ddf3', index=None, type='container_file_citation', container_id='cntr_68314a508e8081919e5d1051f3fcbe27', end_index=0, filename='cfile_68314ba182008191ac8c3503d924ddf3.png', start_index=0)], text='Here is the bar chart showing the breakdown of users by gender in the dataset. The dataset includes several gender categories such as Male, Female, Non-binary, Polygender, Genderqueer, Agender, Bigender, and Genderfluid. If you would like, I can also provide the exact counts or analyze the data further.', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='required', tools=[CodeInterpreter(container='cntr_68314a508e8081919e5d1051f3fcbe27', type='code_interpreter')], top_p=1.0, background=False, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=4219, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=278, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=4497), user=None, store=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream data rather than load all in memory\n",
    "@router.post(\"/upload_csv\", response_model=UploadCSVResponse)\n",
    "async def upload_csv_true_streaming(\n",
    "    file: UploadFile = File(...),\n",
    "    current_user: dict = Depends(get_current_user),\n",
    "    db: Database = Depends(get_db)\n",
    "):\n",
    "    \"\"\"\n",
    "    TRUE STREAMING: Never store the full file in memory!\n",
    "    Stream directly to Azure while getting CSV preview from first chunk.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Validate file type\n",
    "    if not file.filename.endswith('.csv'):\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"Only CSV files are allowed\"\n",
    "        )\n",
    "    \n",
    "    # 2. TRUE STREAMING SETUP\n",
    "    max_size = 30 * 1024 * 1024  # 30MB in bytes\n",
    "    chunk_size = 64 * 1024  # 64KB chunks\n",
    "    azure_block_size = 4 * 1024 * 1024  # 4MB Azure blocks\n",
    "    \n",
    "    total_size = 0\n",
    "    csv_preview_data = None\n",
    "    column_names = None\n",
    "    total_columns = 0\n",
    "    \n",
    "    # Azure setup\n",
    "    session_id = str(uuid.uuid4())\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(settings.BLOB_STORAGE_ACCOUNT_KEY)\n",
    "    container_name = \"images-analysis\"\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    blob_name = f\"{session_id}_{timestamp}_{file.filename}\"\n",
    "    blob_client = blob_service_client.get_container_client(container_name).get_blob_client(blob_name)\n",
    "    \n",
    "    # Azure block upload setup\n",
    "    \n",
    "    block_list = []\n",
    "    current_block_data = bytearray()\n",
    "    block_counter = 0\n",
    "    \n",
    "    print(\"🚀 TRUE STREAMING: Processing file without storing full content...\")\n",
    "    \n",
    "    try:\n",
    "        # Create container if needed\n",
    "        try:\n",
    "            blob_service_client.get_container_client(container_name).create_container()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # TRUE STREAMING LOOP\n",
    "        while True:\n",
    "            # Read one chunk at a time\n",
    "            chunk = await file.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break  # End of file\n",
    "            \n",
    "            total_size += len(chunk)\n",
    "            print(f\"📥 Processing chunk: {len(chunk)} bytes (Total processed: {total_size} bytes)\")\n",
    "            \n",
    "            # Size check - early exit if too big\n",
    "            if total_size > max_size:\n",
    "                print(\"❌ File too large - stopping stream\")\n",
    "                raise HTTPException(\n",
    "                    status_code=400,\n",
    "                    detail=f\"File too large. Maximum size is {max_size//1024//1024}MB\"\n",
    "                )\n",
    "            \n",
    "            # GET CSV PREVIEW FROM FIRST CHUNK ONLY\n",
    "            if csv_preview_data is None and total_size <= chunk_size:\n",
    "                try:\n",
    "                    # Decode first chunk to get CSV structure\n",
    "                    csv_string = chunk.decode('utf-8')\n",
    "                    df = pd.read_csv(StringIO(csv_string), nrows=5)\n",
    "                    \n",
    "                    if df.empty or len(df.columns) == 0:\n",
    "                        raise HTTPException(status_code=400, detail=\"Invalid CSV format\")\n",
    "                    \n",
    "                    # Extract metadata from first chunk\n",
    "                    total_columns = len(df.columns)\n",
    "                    column_names = df.columns.tolist()\n",
    "                    \n",
    "                    # Create preview data\n",
    "                    csv_preview_data = []\n",
    "                    for index, row in df.iterrows():\n",
    "                        row_dict = {}\n",
    "                        for column in df.columns:\n",
    "                            value = row[column]\n",
    "                            if pd.isna(value):\n",
    "                                row_dict[column] = None\n",
    "                            elif isinstance(value, (int, float)):\n",
    "                                row_dict[column] = value.item() if hasattr(value, 'item') else value\n",
    "                            else:\n",
    "                                row_dict[column] = str(value)\n",
    "                        csv_preview_data.append(row_dict)\n",
    "                    \n",
    "                    print(f\"✅ CSV preview extracted from first chunk: {total_columns} columns\")\n",
    "                    del df, csv_string  # Free memory immediately\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error processing CSV preview: {e}\")\n",
    "                    raise HTTPException(status_code=400, detail=\"Invalid CSV format\")\n",
    "            \n",
    "            # ADD CHUNK TO CURRENT AZURE BLOCK\n",
    "            current_block_data.extend(chunk)\n",
    "            \n",
    "            # UPLOAD BLOCK WHEN IT REACHES 4MB OR END OF FILE\n",
    "            if len(current_block_data) >= azure_block_size:\n",
    "                # Upload this block to Azure immediately\n",
    "                block_id = base64.b64encode(f\"block-{block_counter:06d}\".encode()).decode()\n",
    "                \n",
    "                print(f\"📤 Uploading Azure block {block_counter + 1}: {len(current_block_data)} bytes\")\n",
    "                \n",
    "                blob_client.stage_block(\n",
    "                    block_id=block_id,\n",
    "                    data=bytes(current_block_data)\n",
    "                )\n",
    "                \n",
    "                block_list.append(BlobBlock(block_id=block_id))\n",
    "                block_counter += 1\n",
    "                \n",
    "                print(f\"✅ Block uploaded. Memory freed. Total blocks: {len(block_list)}\")\n",
    "                \n",
    "                # CLEAR BLOCK DATA - FREE MEMORY!\n",
    "                current_block_data = bytearray()\n",
    "        \n",
    "        # Upload final block if there's remaining data\n",
    "        if len(current_block_data) > 0:\n",
    "            block_id = base64.b64encode(f\"block-{block_counter:06d}\".encode()).decode()\n",
    "            \n",
    "            print(f\"📤 Uploading final Azure block: {len(current_block_data)} bytes\")\n",
    "            \n",
    "            blob_client.stage_block(\n",
    "                block_id=block_id,\n",
    "                data=bytes(current_block_data)\n",
    "            )\n",
    "            \n",
    "            block_list.append(BlobBlock(block_id=block_id))\n",
    "            print(f\"✅ Final block uploaded\")\n",
    "        \n",
    "        # Commit all blocks to create final blob\n",
    "        print(f\"🔗 Committing {len(block_list)} blocks to create final blob...\")\n",
    "        \n",
    "        blob_client.commit_block_list(\n",
    "            block_list=block_list,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "        \n",
    "        file_url = blob_client.url\n",
    "        \n",
    "        print(f\"✅ TRUE STREAMING COMPLETE!\")\n",
    "        print(f\"📊 Total file size: {total_size} bytes\")\n",
    "        print(f\"📊 Azure blocks created: {len(block_list)}\")\n",
    "        print(f\"💾 Max memory used: ~{azure_block_size//1024//1024}MB (one block)\")\n",
    "        print(f\"🔗 File URL: {file_url}\")\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during streaming: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Error during file processing\")\n",
    "    \n",
    "    # Validate that we got CSV preview\n",
    "    if csv_preview_data is None:\n",
    "        raise HTTPException(status_code=400, detail=\"Could not extract CSV preview\")\n",
    "    \n",
    "    # 3. Save session data to MongoDB\n",
    "    try:\n",
    "        session_document = {\n",
    "            \"session_id\": session_id,\n",
    "            \"user_email\": current_user[\"email\"],\n",
    "            \"user_id\": str(current_user[\"_id\"]),\n",
    "            \"file_info\": {\n",
    "                \"original_filename\": file.filename,\n",
    "                \"blob_name\": blob_name,\n",
    "                \"container_name\": container_name,\n",
    "                \"file_url\": file_url,\n",
    "                \"file_size\": total_size,\n",
    "                \"content_type\": \"text/csv\"\n",
    "            },\n",
    "            \"csv_info\": {\n",
    "                \"total_columns\": total_columns,\n",
    "                \"column_names\": column_names,\n",
    "                \"preview_data\": csv_preview_data\n",
    "            },\n",
    "            \"created_at\": datetime.utcnow(),\n",
    "            \"updated_at\": datetime.utcnow(),\n",
    "            \"status\": \"active\"\n",
    "        }\n",
    "        \n",
    "        sessions_collection = db[\"csv_sessions\"]\n",
    "        result = await sessions_collection.insert_one(session_document)\n",
    "        \n",
    "        if not result.inserted_id:\n",
    "            raise Exception(\"Failed to create session in database\")\n",
    "        \n",
    "        print(f\"✅ MongoDB session created: {session_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Clean up blob if database fails\n",
    "        try:\n",
    "            blob_client.delete_blob()\n",
    "            print(f\"🗑️ Cleaned up blob after database error\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        await log_error(\n",
    "            error=e,\n",
    "            location=\"upload_csv_mongodb\",\n",
    "            additional_info={\"session_id\": session_id, \"user_email\": current_user[\"email\"]}\n",
    "        )\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to create session in database\")\n",
    "    \n",
    "    # 4. Return response\n",
    "    return {\n",
    "        \"session_id\": session_id,\n",
    "        \"file_url\": file_url,\n",
    "        \"file_name\": file.filename,\n",
    "        \"preview_data\": csv_preview_data,\n",
    "        \"file_info\": {\n",
    "            \"original_filename\": file.filename,\n",
    "            \"blob_name\": blob_name,\n",
    "            \"container_name\": container_name,\n",
    "            \"file_url\": file_url,\n",
    "            \"file_size\": total_size,\n",
    "            \"content_type\": \"text/csv\"\n",
    "        },\n",
    "        \"message\": \"CSV file uploaded successfully with true streaming\",\n",
    "        \"success\": True\n",
    "    }\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
